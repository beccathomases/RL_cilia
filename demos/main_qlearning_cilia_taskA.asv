%% Cilia Task A: Tabular Q-learning over 3 Chebyshev curvature coefficients
% Task A (Side Sweep): maximize lateral sweep of the filament
% Objective: maximize span = max_s x(s) - min_s x(s)
% Constraint: discourage wall crossings y(s) < 0 via soft penalty
% Regularization: penalize bending energy âˆ« kappa(s)^2 ds
%
% State: integer coefficient triple c = [c0,c1,c2], each in [-4,4]
% Action: +/- 1 step on one coefficient (6 actions)
%
% Requires (in ../src):
%   coeffs_to_curve_cheb.m
%   cilia_reward_forward.m
%   cilia_state_index.m
%   cilia_index_state.m
%   cilia_step.m   (terminal-reward version, no crash termination)
%   cilia_plot_shape.m

clear; clc; close all; rng(0);
addpath(fullfile(fileparts(mfilename('fullpath')),'..','src'));

%% ---- Geometry / curve model ----
L    = 1.0;
Npts = 400;
N    = 3;        %#ok<NASGU> % number of coefficients (fixed at 3 here)
theta0 = pi/2;

%% ---- Coefficient grid ----
cmin = -4;
cmax =  4;
vals = cmin:cmax;
nVals = numel(vals);           % 9
S = nVals^3;                   % 729 states
A = 6;                         % +/- on each of 3 coeffs

%% ---- RL hyperparameters ----
N_EPISODES = 10000;
H = 25;                        % steps per episode (coefficient updates)

alpha0   = 0.40;
alphaMin = 0.05;
gamma    = 1.0;                % terminal-only reward

eps0   = 0.50;
epsMin = 0.05;

alpha_decay = 0.995;           % keep as-is
eps_decay   = 0.999;           % SLOWER exploration decay (important for sparse terminal reward)

%% ---- Reward weights ----
params.wx = 1.0;               % reward on span = max(x)-min(x)
params.wb = 0.05;              % bending penalty weight
params.ww = 1.0;               % wall penalty weight multiplier
params.mu_wall = 800.0;        % INCREASED wall penalty strength (was 200)
params.theta0 = theta0;        % clamped angle at wall y=0
params.step_cost = 0.01;       % try 0.01 or 0.02 cost f


%% ---- Initialize Q-table ----
Q = zeros(S, A);

% Track learning
ep_return = zeros(N_EPISODES,1);

% Track "best by span" (main objective)
bestSpan = -Inf;
bestC_bySpan = [0;0;0];

% (Optional) also track best by one-step reward value
bestR = -Inf;
bestC_byR = [0;0;0];

%% ---- Training ----
for ep = 1:N_EPISODES

    alpha = max(alphaMin, alpha0 * (alpha_decay)^(ep-1));
    eps   = max(epsMin,   eps0   * (eps_decay)^(ep-1));

    c = [0;0;0];  % start each episode at straight filament
    s = cilia_state_index(c, cmin, cmax);

    G = 0;

    for t = 1:H
        % epsilon-greedy action
        if rand < eps
            a = randi(A);
        else
            [~, a] = max(Q(s,:));
        end

        % step (terminal-only reward inside cilia_step)
        [s2, r, done, c] = cilia_step(s, a, c, cmin, cmax, L, Npts, params, t, H);

        % Q-learning update (TERMINAL-AWARE)
        if done
            td_target = r;  % no bootstrap at terminal
        else
            td_target = r + gamma * max(Q(s2,:));
        end
        Q(s,a) = Q(s,a) + alpha * (td_target - Q(s,a));

        s = s2;
        G = G + r;

        if done
            break;
        end
    end

    % Evaluate current final shape (after episode ends)
    outFinal = coeffs_to_curve_cheb(c, L, Npts, params.theta0, [0;0]);
    [rFinal, infoFinal] = cilia_reward_forward(outFinal, params);

    % Track best by span (main objective)
    if infoFinal.span > bestSpan
        bestSpan = infoFinal.span;
        bestC_bySpan = c;
    end

    % (Optional) also track best by one-step reward
    if rFinal > bestR
        bestR = rFinal;
        bestC_byR = c;
    end

    ep_return(ep) = G;

    if mod(ep, 200) == 0
        recent = ep_return(max(1,ep-199):ep);
        fprintf('Ep %5d | avg return(200): %+0.3f | eps=%0.3f | alpha=%0.3f | last span=%.3f | last minY=%.3f\n', ...
            ep, mean(recent), eps, alpha, infoFinal.span, infoFinal.minY);
    end
end

disp('Training done.');

% Plot best shape found (by span)
cilia_plot_best_shape(bestC_bySpan, bestSpan, L, Npts, params);

%% ---- Evaluate greedy policy from start state ----
c = [0;0;0];
s = cilia_state_index(c, cmin, cmax);

trajC = zeros(3, H+1);
trajC(:,1) = c;

trajR = zeros(H,1);

for t = 1:H
    [~, a] = max(Q(s,:));
    [s2, r, done, c] = cilia_step(s, a, c, cmin, cmax, L, Npts, params, t, H);

    trajC(:,t+1) = c;
    trajR(t) = r;

    s = s2;
    if done, break; end
end

fprintf('\nGreedy rollout coefficients (columns):\n');
disp(trajC(:,1:(t+1)));

%% ---- Plot learning curve ----
figure; plot(ep_return, 'LineWidth', 1);
xlabel('Episode'); ylabel('Return');
title('Cilia Task A: Episode return (terminal reward)');

%% ---- Plot the final greedy shape ----
out = coeffs_to_curve_cheb(c, L, Npts, params.theta0, [0;0]);
[~, info] = cilia_reward_forward(out, params);

figure;
cilia_plot_shape(out);
title(sprintf('Greedy final shape | c=[%d,%d,%d], span=%.3f, minY=%.3f', ...
    c(1), c(2), c(3), info.span, info.minY));

%% ---- Show curvature too ----
figure; plot(out.s, out.kappa, 'LineWidth', 2); grid on;
xlabel('s'); ylabel('\kappa(s)');
title('Curvature of greedy final shape');

%% ---- Plot value slice ----
cilia_plot_value_slice(Q, cmin, cmax, 0);   % slice at c2 = 0

%% ---- Plot coefficient trajectory ----
cilia_plot_coeff_trajectory(trajC);
